{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Configuring-pandas\" data-toc-modified-id=\"Configuring-pandas-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Configuring pandas</a></span></li><li><span><a href=\"#데이터-접근\" data-toc-modified-id=\"데이터-접근-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>데이터 접근</a></span></li><li><span><a href=\"#로컬-파일\" data-toc-modified-id=\"로컬-파일-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>로컬 파일</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#CSV-데이터셋-예제:-msft.csv\" data-toc-modified-id=\"CSV-데이터셋-예제:-msft.csv-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>CSV 데이터셋 예제: msft.csv</a></span></li></ul></li><li><span><a href=\"#Reading-a-CSV-into-a-DataFrame\" data-toc-modified-id=\"Reading-a-CSV-into-a-DataFrame-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Reading a CSV into a DataFrame</a></span></li><li><span><a href=\"#Specifying-the-index-column-when-reading-a-CSV-file\" data-toc-modified-id=\"Specifying-the-index-column-when-reading-a-CSV-file-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Specifying the index column when reading a CSV file</a></span></li><li><span><a href=\"#Data-type-inference-and-specification\" data-toc-modified-id=\"Data-type-inference-and-specification-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Data type inference and specification</a></span></li><li><span><a href=\"#Specifying-column-names\" data-toc-modified-id=\"Specifying-column-names-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Specifying column names</a></span></li><li><span><a href=\"#Specifying-specific-columns-to-load\" data-toc-modified-id=\"Specifying-specific-columns-to-load-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Specifying specific columns to load</a></span></li><li><span><a href=\"#Saving-a-DataFrame-to-a-CSV\" data-toc-modified-id=\"Saving-a-DataFrame-to-a-CSV-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Saving a DataFrame to a CSV</a></span></li><li><span><a href=\"#General-field-delimited-data\" data-toc-modified-id=\"General-field-delimited-data-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>General field-delimited data</a></span></li><li><span><a href=\"#Handling-variants-of-formats-in-field-delimited-data\" data-toc-modified-id=\"Handling-variants-of-formats-in-field-delimited-data-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Handling variants of formats in field-delimited data</a></span><ul class=\"toc-item\"><li><span><a href=\"#(skiprows-=-[])-:-헤더-지우기\" data-toc-modified-id=\"(skiprows-=-[])-:-헤더-지우기-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span><code>(skiprows = [])</code> : 헤더 지우기</a></span></li><li><span><a href=\"#(skipfooter-=-[])-:-푸터-지우기\" data-toc-modified-id=\"(skipfooter-=-[])-:-푸터-지우기-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span><code>(skipfooter = [])</code> : 푸터 지우기</a></span></li><li><span><a href=\"#(nrows-=-)-:-처음-몇-개의-로우만-불러오기\" data-toc-modified-id=\"(nrows-=-)-:-처음-몇-개의-로우만-불러오기-3.8.3\"><span class=\"toc-item-num\">3.8.3&nbsp;&nbsp;</span><code>(nrows = )</code> : 처음 몇 개의 로우만 불러오기</a></span></li></ul></li><li><span><a href=\"#Reading-and-writing-data-in-Excel-format\" data-toc-modified-id=\"Reading-and-writing-data-in-Excel-format-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Reading and writing data in Excel format</a></span><ul class=\"toc-item\"><li><span><a href=\"#pd.read_excel()\" data-toc-modified-id=\"pd.read_excel()-3.9.1\"><span class=\"toc-item-num\">3.9.1&nbsp;&nbsp;</span><code>pd.read_excel()</code></a></span></li><li><span><a href=\"#.to_excel(),-to-xls\" data-toc-modified-id=\".to_excel(),-to-xls-3.9.2\"><span class=\"toc-item-num\">3.9.2&nbsp;&nbsp;</span><code>.to_excel()</code>, to xls</a></span></li><li><span><a href=\"#.to_excel(),-to-xlsx\" data-toc-modified-id=\".to_excel(),-to-xlsx-3.9.3\"><span class=\"toc-item-num\">3.9.3&nbsp;&nbsp;</span><code>.to_excel()</code>, to xlsx</a></span></li></ul></li></ul></li><li><span><a href=\"#Reading-and-writing-JSON-files\" data-toc-modified-id=\"Reading-and-writing-JSON-files-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Reading and writing JSON files</a></span></li><li><span><a href=\"#HTML\" data-toc-modified-id=\"HTML-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>HTML</a></span><ul class=\"toc-item\"><li><span><a href=\"#HTML-데이터-읽기\" data-toc-modified-id=\"HTML-데이터-읽기-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>HTML 데이터 읽기</a></span></li></ul></li><li><span><a href=\"#HDF5(pytable-설치-문제-해결-후-다시-하기로)\" data-toc-modified-id=\"HDF5(pytable-설치-문제-해결-후-다시-하기로)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>HDF5(pytable 설치 문제 해결 후 다시 하기로)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reading-and-writing-HDF5-format-files\" data-toc-modified-id=\"Reading-and-writing-HDF5-format-files-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Reading and writing HDF5 format files</a></span></li></ul></li><li><span><a href=\"#웹-데이터\" data-toc-modified-id=\"웹-데이터-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>웹 데이터</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accessing-data-on-the-web-and-in-the-cloud\" data-toc-modified-id=\"Accessing-data-on-the-web-and-in-the-cloud-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Accessing data on the web and in the cloud</a></span></li></ul></li><li><span><a href=\"#Reading-and-writing-from/to-SQL-databases\" data-toc-modified-id=\"Reading-and-writing-from/to-SQL-databases-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Reading and writing from/to SQL databases</a></span></li><li><span><a href=\"#Reading-stock-data-from-Google-Finance\" data-toc-modified-id=\"Reading-stock-data-from-Google-Finance-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Reading stock data from Google Finance</a></span></li><li><span><a href=\"#Retrieving-options-data-from-Google-Finance\" data-toc-modified-id=\"Retrieving-options-data-from-Google-Finance-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Retrieving options data from Google Finance</a></span></li><li><span><a href=\"#Reading-economic-data-from-the-Federal-Reserve-Bank-of-St.-Louis\" data-toc-modified-id=\"Reading-economic-data-from-the-Federal-Reserve-Bank-of-St.-Louis-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Reading economic data from the Federal Reserve Bank of St. Louis</a></span></li><li><span><a href=\"#Accessing-Kenneth-French-data\" data-toc-modified-id=\"Accessing-Kenneth-French-data-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Accessing Kenneth French data</a></span></li><li><span><a href=\"#Reading-from-the-World-Bank\" data-toc-modified-id=\"Reading-from-the-World-Bank-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Reading from the World Bank</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n",
      "1.22.4\n",
      "3.8.5 (default, Sep  4 2020, 02:22:02) \n",
      "[Clang 10.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "# import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# used for dates\n",
    "import datetime\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Set some pandas options controlling output format\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 60)\n",
    "\n",
    "# bring in matplotlib for graphics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# version check\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 접근\n",
    "---\n",
    "현실에서의 데이터 분석에 있어서 거의 대부분은 외부로부터의 데이터 로딩이 필요하다. pandas는 파이썬을 기반으로 구현됐으므로, 데이터를 가져오기 위해 파이썬이 제공하는 모든 수단을 이용할 수 있다!!! 또한 엑셀 스프레드시트, 웹사이트, 웹서비스, 데이터베이스, 클라우드 서비스 등 거의 제약 없이 어떤 소스의 데이터라도 접근이 가능하다.  \n",
    "그러나 파이썬의 표준 데이터 로딩 함수를 사용하는 경우에는 파이썬 객체를 pandas의 `Series`나 `DataFrame` 객체로 변환하는 작업이 필요하며, 이는 소스코드의 복잡도를 증가시킨다. 그런 복잡함을 경감해주기위해 pandas는 데이터를 곧바로 pandas 객체로 로딩하는 다양한 기능을 제공한다.  \n",
    "\n",
    "로컬 파일  \n",
    "- CSV를 데이터 프레임으로 로딩\n",
    "- CSV 로딩 시 인덱스 칼럼 지정\n",
    "- 데이터 타입의 추론과 지정\n",
    "- 칼럼명 지정\n",
    "- 특정 칼럼의 로딩\n",
    "- 데이터를 CSV로 저장\n",
    "- 필드 구분 데이터로 작업\n",
    "- 필드 구분 데이터의 다양한 형식 다루기\n",
    "- 엑셀 데이터의 읽기와 쓰기  \n",
    "\n",
    "웹 파일\n",
    "- JSON 파일의 읽기와 쓰기\n",
    "- HTML 데이터 읽기\n",
    "- HDF5 파일의 읽기와 쓰기\n",
    "- 웹을 통한 CSV 데이터 접근\n",
    "- 데이터베이스의 읽기와 쓰기\n",
    "- 야후!와 구글로부터 주식 데이터 읽기\n",
    "- 구글파이낸스의 옵션 데이터 가져오기\n",
    "- 세인트루이스 연방준비은행의 FRED 데이터 가져오기\n",
    "- 케네스 프렌치 데이터에 접근\n",
    "- 세계은행의 데이터 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로컬 파일\n",
    "---\n",
    "CSV, 텍스트, 테이블 형식의 데이터\n",
    "\n",
    "CSV는 pandas로 작업할 때 가장 흔히 다루게 되는 포맷이다. 많은 웹 기반의 서비스가 데이터를 CSV 형식으로 제공하며, 이는 기업 내 정보 시스템의 경우도 마찬가지다.  \n",
    "CSV는 사용하기 쉬운 포맷이며, Excel 등과 같은 스프레드 시트에서 내보내기 형식으로도 자주 사용된다.  \n",
    "CSV는 여러 줄을 가질 수 있는 텍스트 기반의 데이터이다. 각 값은 쉼표로 구분된다.  \n",
    "마치 스프레드시트에서 하나의 시트처럼 CSV를 하나의 데이터 테이블로 생각할 수 있다. 즉, 각각의 줄이 데이터의 로우이며, 쉼표로 구분된 텍스트 값들이 해당 로우의 각 칼럼이 된다.  \n",
    "\n",
    "    CSV = comma separated values\n",
    "\n",
    "CSV를 통해 배운 내용은 이후의 다른 포맷을 다룰 때에도 적용될 뿐만 아니라 약간의 편의성도 더해준다.\n",
    "\n",
    "### CSV 데이터셋 예제: msft.csv \n",
    "MSTF 티커(ticker, 시황)의 주가 현황 데이터다.  \n",
    "\n",
    "`!head` 명령어로 원하는 수의 줄을 읽을 수 있다.  \n",
    "윈도우의 경우`type` 명령어를 사용해야 한다.  \n",
    "\n",
    "첫째 로우는 쉼표로 구분된 칼럼명으로 구성되어있다. 이후 각 로우는 특정 날짜에서의 주가 데이터로 이루어져있다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date,Open,High,Low,Close,Volume\r\n",
      "7/21/2014,83.46,83.53,81.81,81.93,2359300\r\n",
      "7/18/2014,83.3,83.4,82.52,83.35,4020800\r\n",
      "7/17/2014,84.35,84.63,83.33,83.63,1974000\r\n",
      "7/16/2014,83.77,84.91,83.66,84.91,1755600\r\n"
     ]
    }
   ],
   "source": [
    "# view the first five lines of data/msft.csv\n",
    "!head -n 5 /Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv # mac or Linux\n",
    "# type data/msft.csv # on windows, but shows the entire file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a CSV into a DataFrame\n",
    "CSV를 데이터 프레임으로 로딩\n",
    "\n",
    "'mstf.csv' 데이터는 모든 데이터가 완전하고, 첫 번째 로우가 칼럼명으로 되어있기 때문에 완벽하게 데이터 프레임으로 불러올 수 있다.  \n",
    "\n",
    "```\n",
    "pd.read_csv('location')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close   Volume\n",
       "0  7/21/2014  83.46  83.53  81.81  81.93  2359300\n",
       "1  7/18/2014  83.30  83.40  82.52  83.35  4020800\n",
       "2  7/17/2014  84.35  84.63  83.33  83.63  1974000\n",
       "3  7/16/2014  83.77  84.91  83.66  84.91  1755600\n",
       "4  7/15/2014  84.30  84.38  83.20  83.58  1874700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in msft.csv into a DataFrame\n",
    "msft = pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\")\n",
    "msft[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the index column when reading a CSV file\n",
    "CSV 로딩 시 인덱스 칼럼 지정\n",
    "\n",
    "앞의 예제에서는 날짜가 아닌 0에서 시작하는 숫자가 인덱스였다. 이는 pandas가 파일의 특정 칼럼을 반드시 인덱스로 사용해야 한다고 제약하지 않기 때문인데,  \n",
    "`read_csv()` 사용시 `index_col = (제로베이스 포지션)` 파라미터를 사용하여 원하는 칼럼을 인덱스로 사용할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Open   High    Low  Close   Volume\n",
       "Date                                          \n",
       "7/21/2014  83.46  83.53  81.81  81.93  2359300\n",
       "7/18/2014  83.30  83.40  82.52  83.35  4020800\n",
       "7/17/2014  84.35  84.63  83.33  83.63  1974000\n",
       "7/16/2014  83.77  84.91  83.66  84.91  1755600\n",
       "7/15/2014  84.30  84.38  83.20  83.58  1874700"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use column 0 as the index\n",
    "msft = pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\", index_col= 0)\n",
    "msft[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type inference and specification\n",
    "데이터 타입의 추론과 지정\n",
    "\n",
    "칼럼의 데이터 타입을 확인해보면 pandas가 해당 데이터를 통해 추론한 칼럼 타입을 알 수 있다.  \n",
    "강제로 지정하려면 **`pd.read_csv(dtype = )` 파라미터를 사용한다. 딕셔너리 형태로** 지정해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open      float64\n",
       "High      float64\n",
       "Low       float64\n",
       "Close     float64\n",
       "Volume      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the types of the columns in this DataFrame\n",
    "msft.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open      float64\n",
       "High      float64\n",
       "Low       float64\n",
       "Close     float64\n",
       "Volume    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify that the Volume column should be a float64\n",
    "msft = pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\", \n",
    "                    dtype= {'Volume': np.float64},\n",
    "                    index_col= 0)\n",
    "msft.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying column names\n",
    "칼럼명 지정\n",
    "\n",
    "`pd.read_csv(names = )` 파라미터를 사용하면 데이터를 읽어올 때 칼럼명도 직접 지정할 수 있다.  \n",
    "**이때 `header = 0` 파라미터를 사용하여 파일의 첫 번째 로우를 건너 뛰도록** 해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        date   open   high    low  close   volume\n",
       "0  7/21/2014  83.46  83.53  81.81  81.93  2359300\n",
       "1  7/18/2014  83.30  83.40  82.52  83.35  4020800\n",
       "2  7/17/2014  84.35  84.63  83.33  83.63  1974000\n",
       "3  7/16/2014  83.77  84.91  83.66  84.91  1755600\n",
       "4  7/15/2014  84.30  84.38  83.20  83.58  1874700"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify a new set of names for the columns\n",
    "# all lower case, remove space in Adj Close\n",
    "# also, header=0 skips the header row\n",
    "df = pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\", \n",
    "                 header= 0,\n",
    "                 names= ['date', 'open', 'high', 'low', \n",
    "                        'close', 'volume'])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying specific columns to load\n",
    "특정 칼럼만 로드하기\n",
    "\n",
    "파일을 읽을 때 로딩하고자 하는 칼럼을 지정할 수도 있다. 이는 분석 대상이 아닌 칼럼이 파일에 많은 경우 로드와 세이브에 필요한 시간과 메모리를 절약할 수 있는 유용한 방법이다.  \n",
    "`pd.read_csv(usecols = )` 파라미터로 지정할 수 있으며, 이 파라미터에는 칼럼 이름(헤더에 포함된 칼럼명)이나 오프셋(제로베이스 포지션)의 리스트를 전달하면 된다.  \n",
    "\n",
    "날짜와 종가만 읽어들여 날짜를 인덱스로 사용한다면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Close\n",
       "Date            \n",
       "7/21/2014  81.93\n",
       "7/18/2014  83.35\n",
       "7/17/2014  83.63\n",
       "7/16/2014  84.91\n",
       "7/15/2014  83.58"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data only in the Date and Close columns\n",
    "# and index by the Date column\n",
    "df2 = pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\", \n",
    "                  usecols= ['Date', 'Close'], \n",
    "                  index_col= ['Date'])\n",
    "df2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Close\n",
       "Date            \n",
       "7/21/2014  81.93\n",
       "7/18/2014  83.35\n",
       "7/17/2014  83.63\n",
       "7/16/2014  84.91\n",
       "7/15/2014  83.58"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\", \n",
    "                  usecols= [0,4], \n",
    "                  index_col= [0])\n",
    "df2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a DataFrame to a CSV\n",
    "데이터 프레임을 CSV 파일로 저장 \n",
    "\n",
    "`to_csv()` 메서드를 사용하면 `DataFrame`을 cSV 파일로 저장할 수 있다.  \n",
    "\n",
    "이때 `index_label`같이 특정 칼럼을 인덱스 레이블로 지정하는 것이 좋다.  \n",
    "그렇지 않으면 파일의 첫 번째 로우에 인덱스 이름이 포함되지 않으므로, 나중에 이 파일을 읽을 때 어려움이 있을 수 있다.  \n",
    "\n",
    "(추가로, 정수 인덱스가 pandas에서 부여된 상황일 때 `index = False`로 하면 불필요한 인덱스 칼럼이 포함되는 것을 막을 수 있다.)\n",
    "\n",
    "파일이 제대로 저장됐는지 알기 위해 `!head` 명령어로 콘텐츠의 일부를 확인해 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df2 to a new csv file\n",
    "# also specify naming the index as date\n",
    "df2.to_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft_modified.csv\", index_label= 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date,Close\r\n",
      "7/21/2014,81.93\r\n",
      "7/18/2014,83.35\r\n",
      "7/17/2014,83.63\r\n",
      "7/16/2014,84.91\r\n"
     ]
    }
   ],
   "source": [
    "# view the start of the file just saved\n",
    "!head -n 5 /Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft_modified.csv\n",
    "#type data/msft_modified.csv # windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General field-delimited data\n",
    "필드 구분ㄴ 데이터로 작업\n",
    "\n",
    "사실 CSV는 필드 구분(field-delimited) 데이터의 특정 형태 중 하나이다.  \n",
    "필드 구분 데이터에서 각 아이템은 기호(구분자)에 의해 구분되며, CSV의 경우 그 기호가 , 인 것이다.  \n",
    "쉽표 대신 파이프(|) 기호를 사용할 수 있으며, 그렇게 구분한 데이터를 파이프 구분(pipe-delimited) 데이터라고 부른다.\n",
    "\n",
    "필드 구분 데이터를 읽기 위해서 `pd.read_table(sep = )` 을 사용할 수 있다. \n",
    "구분자를 지정하여 파일을 읽어올 수 있다.  \n",
    "(CSV파일은 , 로 구분된 필드 구분 데이터이기 때문에 구분자를 쉼표로 지정하고 로드 할 수 있다. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close   Volume\n",
       "0  7/21/2014  83.46  83.53  81.81  81.93  2359300\n",
       "1  7/18/2014  83.30  83.40  82.52  83.35  4020800\n",
       "2  7/17/2014  84.35  84.63  83.33  83.63  1974000\n",
       "3  7/16/2014  83.77  84.91  83.66  84.91  1755600\n",
       "4  7/15/2014  84.30  84.38  83.20  83.58  1874700"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use read_table with sep=',' to read a CSV\n",
    "df = pd.read_table(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\", sep= ',')\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.to_table()`이라는 메서드는 없고, `.to_csv(sep = )`으로 지정해 필드 구분 데이터를 저장할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Date|Open|High|Low|Close|Volume\r\n",
      "0|7/21/2014|83.46|83.53|81.81|81.93|2359300\r\n",
      "1|7/18/2014|83.3|83.4|82.52|83.35|4020800\r\n",
      "2|7/17/2014|84.35|84.63|83.33|83.63|1974000\r\n",
      "3|7/16/2014|83.77|84.91|83.66|84.91|1755600\r\n"
     ]
    }
   ],
   "source": [
    "# save as pipe delimited\n",
    "df.to_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft_piped.txt\", sep='|')\n",
    "# check that it worked\n",
    "!head -n 5 /Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft_piped.txt # osx or Linux\n",
    "# type data/psft_piped.txt # on windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling variants of formats in field-delimited data\n",
    "필드 구분 데이터의 다양한 형식 다루기\n",
    "\n",
    "필드 구분 데이터에는 헤더와 푸터가 추가로 포함될 수 있다.  \n",
    "예컨대 헤더에는 기업 정보가 있을 수 있고, 푸터에는 접수 번호, 주소, 요약 등의 있을 수 있다.  \n",
    "또는 줄 사이사이에 그런 데이터가 들어 있을 수도 있다.  \n",
    "`pd.read_csv()`, `pd.read_table()` 메서드에는 그런 상황에 대처할 수 있는 파라미터들이 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is fun because the data does not start on the first line,,,,,\r\n",
      "Date,Open,High,Low,Close,Volume\r\n",
      ",,,,,\r\n",
      "And there is space between the header row and data,,,,,\r\n",
      "7/21/2014,83.46,83.53,81.81,81.93,2359300\r\n",
      "7/18/2014,83.3,83.4,82.52,83.35,4020800\r\n"
     ]
    }
   ],
   "source": [
    "# messy file\n",
    "!head -n 6 /Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft2.csv # osx or Linux\n",
    "# type data/msft2.csv # windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `(skiprows = [])` : 헤더 지우기\n",
    "이런 상황에서 `pd.read_csv(skiprows = [])` 파라미터를 사용할 수 있으며, 0번부터 시작하는 로우 넘버를 사용하여 건너뛰도록 지정할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close   Volume\n",
       "0  7/21/2014  83.46  83.53  81.81  81.93  2359300\n",
       "1  7/18/2014  83.30  83.40  82.52  83.35  4020800\n",
       "2  7/17/2014  84.35  84.63  83.33  83.63  1974000\n",
       "3  7/16/2014  83.77  84.91  83.66  84.91  1755600\n",
       "4  7/15/2014  84.30  84.38  83.20  83.58  1874700"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read, but skip rows 0, 2 and 3\n",
    "df = pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft2.csv\", skiprows= [0, 2, 3])\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뒤에 콘텐츠가 포함되어있는 경우를 본다면, `!head` 가 아니라 `!cat`으로 볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date,Open,High,Low,Close,Volume\r\n",
      "7/21/2014,83.46,83.53,81.81,81.93,2359300\r\n",
      "7/18/2014,83.3,83.4,82.52,83.35,4020800\r\n",
      "\r\n",
      "Uh oh, there is stuff at the end.\r\n"
     ]
    }
   ],
   "source": [
    "# another messy file, with the mess at the end\n",
    "!cat /Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft_with_footer.csv # osx or Linux\n",
    "# type data/msft_with_footer.csv # windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `(skipfooter = [])` : 푸터 지우기\n",
    "\n",
    "이런 경우 `pd.read_csv(skipfooter = )`파라미터에 파일의 마지막 몇 줄을 건너뛸 것인지 지어함으로써 해결할 수 있다. (skip_footer는 지원 중단됨.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close   Volume\n",
       "0  7/21/2014  83.46  83.53  81.81  81.93  2359300\n",
       "1  7/18/2014  83.30  83.40  82.52  83.35  4020800"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skip only two lines at the end\n",
    "df = pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft_with_footer.csv\", \n",
    "                 skipfooter= 2,\n",
    "                 engine= 'python')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    `engine = 'python' :?\n",
    "    pandas에서 engine 옵션의 기본 값은 'c'인데, 아나콘다는 C엔진을 구현하지 않았다. 따라서 이 옵션을 지정하지 않으면 차선책인 파이썬 엔진이 사용되지만, 그와 동시에 경고 메시지도 나타나게 된다. \n",
    "    \n",
    "### `(nrows = )` : 처음 몇 개의 로우만 불러오기\n",
    "\n",
    "큰 파일에서 첫 몇개의 로우만 로딩하고 싶은 경우 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close   Volume\n",
       "0  7/21/2014  83.46  83.53  81.81  81.93  2359300\n",
       "1  7/18/2014  83.30  83.40  82.52  83.35  4020800\n",
       "2  7/17/2014  84.35  84.63  83.33  83.63  1974000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only process the first three rows\n",
    "pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\", nrows= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skiprows` 파라미터를 사용하면 지정한 개수의 로우를 건너뛰고 그 다음 로우만 로딩할 수 있다.  \n",
    "예를 들어 100개의 로우를 건너뛰고, 그 다음 5개 로우만 불러온다면 이렇게 쓸 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        date   open   high    low  close      vol\n",
       "0   3/3/2014  80.35  81.31  79.91  79.97  5004100\n",
       "1  2/28/2014  82.40  83.42  82.17  83.42  2853200\n",
       "2  2/27/2014  84.06  84.63  81.63  82.00  3676800\n",
       "3  2/26/2014  82.92  84.03  82.43  83.81  2623600\n",
       "4  2/25/2014  83.80  83.80  81.72  83.08  3579100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skip 100 lines, then only process the next five\n",
    "pd.read_csv(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft.csv\", \n",
    "            skiprows= 100, nrows= 5, \n",
    "            header= 0,\n",
    "            names= ['date', 'open', 'high', 'low', \n",
    "                   'close', 'vol']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing data in Excel format\n",
    "엑셀 데이터의 읽기와 쓰기\n",
    "\n",
    "### `pd.read_excel()`\n",
    "pandas는 `pd.read_excel()` 함수나 `ExcelFile` 클래스를 통해 엑셀 2003과 그 이후 버전의 엑셀 데이터 로딩을 지원한다. 내부적으로 두 기술 모두 XLPD나 OPENpyXL 패키지를 사용하므로 파이썬 환경에 둘 중 하나가 설치돼 있어야 한다.  \n",
    "\n",
    "엑셀 파일에는 시트가 나누어 존재하기 때문에 불러오면 첫 번째 워크시트의 콘텐츠만 로딩되게 된다.  \n",
    "만약 다른 워크시트를 로딩하려면 `sheet_name = 'worksheetname' `파라미터에 해당 워크 시트 이름을 전달한다.  \n",
    "(버전의 문제로 sheetname이 아니라 sheet_name 이라고 써야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close   Volume\n",
       "0 2014-07-21  83.46  83.53  81.81  81.93  2359300\n",
       "1 2014-07-18  83.30  83.40  82.52  83.35  4020800\n",
       "2 2014-07-17  84.35  84.63  83.33  83.63  1974000\n",
       "3 2014-07-16  83.77  84.91  83.66  84.91  1755600\n",
       "4 2014-07-15  84.30  84.38  83.20  83.58  1874700"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read excel file\n",
    "# only reads first sheet (msft in this case)\n",
    "df = pd.read_excel(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks.xlsx\")\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close    Volume\n",
       "0 2014-07-21  94.99  95.00  93.72  93.94  38887700\n",
       "1 2014-07-18  93.62  94.74  93.02  94.43  49898600\n",
       "2 2014-07-17  95.03  95.28  92.57  93.09  57152000\n",
       "3 2014-07-16  96.97  97.10  94.74  94.78  53396300\n",
       "4 2014-07-15  96.80  96.85  95.03  95.32  45477900"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from the aapl worksheet\n",
    "aapl = pd.read_excel(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks.xlsx\", \n",
    "                     sheet_name= 'aapl')\n",
    "aapl[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.read_csv()`처럼 여기도 pandas가 알아서 칼럼명, 데이터 타입, 인덱스 등을 추론해 적용했다. 그때 사용하던 옵션을 그대로 사용할 수 있다.  \n",
    "\n",
    "\n",
    "### `.to_excel()`, to xls\n",
    "엑셀 파일은 데이터 프레임의 `.to_excel()` 메서드를 사용해 작성할 수 있다. XLS 포맷 작성을 위해서는 XLWT 패키지가 파이썬 환경에 설치돼 있어야 한다.  \n",
    "이미 보유한 데이터 프레임을 저장하는 예시인데, sheet1 으로 자동적으로 저장되게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting XLWT\n",
      "  Downloading xlwt-1.3.0-py2.py3-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: XLWT\n",
      "Successfully installed XLWT-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install XLWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/j72d9nwn5dq0vxtqtw2bbtnh0000gp/T/ipykernel_21942/2780765029.py:2: FutureWarning: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. This is the only engine in pandas that supports writing in the xls format. Install openpyxl and write to an xlsx file instead. You can set the option io.excel.xls.writer to 'xlwt' to silence this warning. While this option is deprecated and will also raise a warning, it can be globally set and the warning suppressed.\n",
      "  df.to_excel(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks2.xls\")\n"
     ]
    }
   ],
   "source": [
    "# save to an .XLS file, in worksheet 'Sheet1'\n",
    "df.to_excel(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks2.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/j72d9nwn5dq0vxtqtw2bbtnh0000gp/T/ipykernel_21942/294439127.py:2: FutureWarning: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. This is the only engine in pandas that supports writing in the xls format. Install openpyxl and write to an xlsx file instead. You can set the option io.excel.xls.writer to 'xlwt' to silence this warning. While this option is deprecated and will also raise a warning, it can be globally set and the warning suppressed.\n",
      "  df.to_excel(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks_msft.xls\", sheet_name= 'MSFT')\n"
     ]
    }
   ],
   "source": [
    "# write making the worksheet name MSFT\n",
    "df.to_excel(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks_msft.xls\", sheet_name= 'MSFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미래의 판다스 패키지에서는 XLWT 가 사라질 것을 경고하고 있다. 그리고 온리 xls 포맷만 지원할 것으로 써있다. XLSX 하일을 쓰려면 openpyxl 을 사용하라는 경고이다. \n",
    "\n",
    "stocks_msft.xls 파일을 열어보면 MSFT 라는 시트 이름으로 데이터가 저장되어 있다! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "둘 이상의 `DataFrame`을 하나의 엑셀 파일에 개별 워크시트로 저장하려면 `with` 키워드롸 함께 `ExcelWriter`를 사용해야 한다.  \n",
    "`ExcelWriter`는 pandas의 일부이긴 하지만, 최상위 네임스페이스에는 속하지 않으므로 반드시 임포트를 해야한다.  \n",
    "\n",
    "다음은 두 개의 ₩DataFrame₩ 객체를 하나의 엑셀 파일에 각각의 워크시트로 저장하는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/j72d9nwn5dq0vxtqtw2bbtnh0000gp/T/ipykernel_21942/2945682119.py:4: FutureWarning: As the xlwt package is no longer maintained, the xlwt engine will be removed in a future version of pandas. This is the only engine in pandas that supports writing in the xls format. Install openpyxl and write to an xlsx file instead. You can set the option io.excel.xls.writer to 'xlwt' to silence this warning. While this option is deprecated and will also raise a warning, it can be globally set and the warning suppressed.\n",
      "  with ExcelWriter(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/all_stocks.xls\") as writer:\n"
     ]
    }
   ],
   "source": [
    "# write multiple sheets\n",
    "# requires use of the ExcelWriter class\n",
    "from pandas import ExcelWriter\n",
    "with ExcelWriter(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/all_stocks.xls\") as writer:\n",
    "    aapl.to_excel(writer, sheet_name='AAPL')\n",
    "    df.to_excel(writer, sheet_name='MSFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.to_excel()`, to xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to xlsx\n",
    "df.to_excel(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/msft2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing JSON files\n",
    "---\n",
    "JSON 파일의 읽기와 쓰기\n",
    "\n",
    "pandas는 JSON(JavaScript Object Notation)포맷으로 저장된 데이터도 읽고 쓸 수 있다.  \n",
    "JSON은 거의 모든 플랫폼과 프로그래밍 언어에서 사용이 가능하여 매우 인기있는 데이터 포맷 중 하나이다. 실제로 웹에서 불러올 때에도 자주 볼 수 있다.  \n",
    "\n",
    "JSON 데이터를 다뤄보기 전에 이미 갖고 있는 엑셀 데이터를 JSON 파일로 저장해본다.  \n",
    "\n",
    "JSON 기반의 데이터는 pd.read_json() 함수를 사용해 읽어들일 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Date\":{\"0\":1405900800000,\"1\":1405641600000,\"2\":1405555200000,\"3\":1405468800000,\"4\":1405382400000},\"Open\":{\"0\":83.46,\"1\":83.3,\"2\":84.35,\"3\":83.77,\"4\":84.3},\"High\":{\"0\":83.53,\"1\":83.4,\"2\":84.63,\"3\":84.91,\"4\":84.38},\"Low\":{\"0\":81.81,\"1\":82.52,\"2\":83.33,\"3\":83.66,\"4\":83.2},\"Close\":{\"0\":81.93,\"1\":83.35,\"2\":83.63,\"3\":84.91,\"4\":83.58},\"Volume\":{\"0\":2359300,\"1\":4020800,\"2\":1974000,\"3\":1755600,\"4\":1874700}}"
     ]
    }
   ],
   "source": [
    "# wirite the excel data to a JSON file\n",
    "df[:5].to_json(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks.json\")\n",
    "!cat /Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks.json # osx or Linux\n",
    "\n",
    "\n",
    "#type data/stocks.json # windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Date   Open   High    Low  Close   Volume\n",
       "0 2014-07-21  83.46  83.53  81.81  81.93  2359300\n",
       "1 2014-07-18  83.30  83.40  82.52  83.35  4020800\n",
       "2 2014-07-17  84.35  84.63  83.33  83.63  1974000\n",
       "3 2014-07-16  83.77  84.91  83.66  84.91  1755600\n",
       "4 2014-07-15  84.30  84.38  83.20  83.58  1874700"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data in from JSON\n",
    "df_from_json = pd.read_json(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks.json\")\n",
    "df_from_json[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML \n",
    "---\n",
    "## HTML 데이터 읽기\n",
    "\n",
    "pandas는 HTML 파일이나 URL로부터의 데이터 로딩을 지원한다. 이를 위해 내부적으로는 LXML, Html5Lib, BeautifulSoup4 패키지를 활용하며, 이들 패키지는 HTML 테이블을 읽거나 쓰는 데 있어서 탁월한 기능을 제공한다.  \n",
    "\n",
    "HTML  관련 함수를 이용할 때 에러가 발생한다면 아나콘다 내비게이터를 이용하도록."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BeautifulSoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: soupsieve, BeautifulSoup4\n",
      "Successfully installed BeautifulSoup4-4.11.1 soupsieve-2.3.2.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.read_html()` 함수는 파일이나 URL로부터 HTML을 읽어 콘텐츠에 포함된 모든 HTML 테이블을 파싱해 하나 이상의 pandas `DataFrame` 객체로 만들어준다.  \n",
    "따라서 이 함수는 항상 `DataFrame` 객체 리스트를 반환한다. (HTML에 포함된 테이블의 개수에 따라 데이터 프레임은 없거나, 하나 이상이다.)\n",
    "\n",
    "(이 책이 만들어 질때에는 페이지가 달랐던 것으로 판단되어, 직접 url을 방문하여 가져왔다.)  \n",
    "```\n",
    "<meta property=\"og:url\" content=\"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\" />\n",
    "```\n",
    "\n",
    "\n",
    "또한 `.to_html()` 메서드를 사용하여 DataFrame을 HTML 파일로 저장할 수 있다. 이 메서드는 전체 HTML 문서가 아닌 <table> 태그만 포함하는 파일을 생성한다.  \n",
    "바로 이어서, 읽었던 주식 데이터를 HTML로 저장해보기로 한다.  \n",
    "    \n",
    "이는 HTML 프래그먼트(fregment)를 pandas로 작성하고 필요할 때 다시 수행함으로써 복잡한 쿼리나 서비스 호출 대신 간편하게 새 데이터로 웹 사이트를 갱신할 수 있다는 점에서 유용하다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the URL to read\n",
    "# url = \"http://www.fdic.gov/bank/individual/failed/banklist.html\"\n",
    "url = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\"\n",
    "# read it\n",
    "banks = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                      Bank NameBank           CityCity\n",
       "0                 Almena State Bank             Almena\n",
       "1        First City Bank of Florida  Fort Walton Beach\n",
       "2              The First State Bank      Barboursville\n",
       "3                Ericson State Bank            Ericson\n",
       "4  City National Bank of New Jersey             Newark"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine a subset of the first table read\n",
    "banks[0][0:5].iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"1\" class=\"dataframe\">\r\n",
      "  <thead>\r\n",
      "    <tr style=\"text-align: right;\">\r\n",
      "      <th></th>\r\n",
      "      <th>Date</th>\r\n",
      "      <th>Open</th>\r\n",
      "      <th>High</th>\r\n",
      "      <th>Low</th>\r\n",
      "      <th>Close</th>\r\n",
      "      <th>Volume</th>\r\n",
      "    </tr>\r\n",
      "  </thead>\r\n"
     ]
    }
   ],
   "source": [
    "# read the stock data\n",
    "df = pd.read_excel(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks.xlsx\")\n",
    "\n",
    "# write the first two rows to HTML\n",
    "df.head(2).to_html(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks.html\")\n",
    "\n",
    "# check the first 28 lines of the output\n",
    "!head -n 12 /Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/stocks.html # max or Linux\n",
    "\n",
    "\n",
    "# type data/stocks.html # window, but prints the entire file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5(pytable 설치 문제 해결 후 다시 하기로)\n",
    "---\n",
    "\n",
    "## Reading and writing HDF5 format files\n",
    "HDF5 파일의 읽기와 쓰기\n",
    "\n",
    "HDF5는 데이터를 저장하고 관리하기 위한 데이터 모델이자 라이브러리이며, 파일 포맷이다.  \n",
    "HDF5는 특히 계산 과학(scientific computing) 분야에서 자주 사용된다. 거의 무제한의 데이터 타입을 지원하며, 유연하고 효율적인 I/O와 대용량의 복잡한 데이터를 위해 설계됐기 때문이다.  \n",
    "\n",
    "HDF5의 높은 이식성과 확장성으로 인해 HDF5의 적용 분야는 발전했다. HDF5 technology Suite에는 HDF5 포맷으로 된 데이터의 관리, 조작, 보기, 분석을 위한 도구와 애플리케이션이 포함되어 있다.  \n",
    "HDF5의 정체는 다음과 같다.  \n",
    "\n",
    "- 매우 복잡한 데이터 객체와 매우 다양한 메타데이터를 표현할 수 있는 다목적의 데이터 모델이다. \n",
    "- 데이터의 크기나 개수에 제한이 없는 완전히 이식 가능한 파일 포맷이다. \n",
    "- 노트북에서 병렬 시스템까지 광범위한 컴퓨터 플랫폼에서 실행될 수 있으며, C, C++, Fortran 90, Java 등 다양한 언어로 고수준 API를 구현한 소프트웨어 라이브러리다. \n",
    "- 데이터 접근 시간의 단축이나 저장 공간의 최적화 등 성능과 관련한 통합된 기능을 제공한다. \n",
    "- 데이터의 관리, 조작, 보기, 분석을 위한 도구와 애플리케이션을 제공한다.  \n",
    "\n",
    "\n",
    "    참고 : https://wikidocs.net/33122   \n",
    "    HDF5를 이해하는 가장 중요한 개념은 그룹(Group), 데이터셋(Dataset), 속성(attribute)이다. 디렉토리 구조와 비슷한데, **그룹=디렉토리, 데이터셋=파일**로 이해하면 쉽다. 속성은 일종의 메타데이터로 그룹이나 데이터셋을 부연 설명하는 것을 의미한다. HDF5 파일을 생성하면 먼저 `/`라는 루트 그룹이 생성되고 그 하위에 트리 구조로 다른 그룹을 생성할 수 있다. 그룹하위에 다른 그룹이 있을 수도 있고, 데이터셋이 존재할 수도 있다. 즉 완전히 운영체계의 디렉토리-파일 구조와 일치한다. 또 다른 특징은 속성인데 속성은 데이터셋이나 그룹을 설명하는데 사용하는데 이를 사용자가 정의하게 된다. 정리하면 HDF5는 Hierarchical Data Format이며 self-describing이 되는 고성능 데이터포맷 또는 DB 정도로 이해할 수 있다. 운영체계와 무관하게 사용할 수 있으며, 대용량 데이터를 빠르게 읽고 쓸 수 있다.\n",
    "\n",
    "<br>\n",
    "HDFStore는 HDF5 포맷을 읽거나 쓰기 위한 딕셔너리와 비슷하며, 계층 구조를 갖는 pandas 객체다. HDFStore는 내부적으로 PyTables 라이브러리를 사용하므로 파이썬 환경에 설치돼 있어야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!pip install pytable\n",
    "```\n",
    "\n",
    "```\n",
    "Collecting pytable\n",
    "  Downloading pytable-0.8.23a.zip (264 kB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.8/264.8 kB 5.0 MB/s eta 0:00:00a 0:00:01\n",
    "  Preparing metadata (setup.py) ... done\n",
    "  Downloading pytable-0.8.22a.zip (264 kB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 kB 4.4 MB/s eta 0:00:00a 0:00:01\n",
    "  Preparing metadata (setup.py) ... done\n",
    "  Downloading pytable-0.8.21a.tar.gz (200 kB)\n",
    "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.1/200.1 kB 5.2 MB/s eta 0:00:00\n",
    "  Preparing metadata (setup.py) ... done\n",
    "ERROR: Cannot install pytable==0.8.21a0, pytable==0.8.22a0 and pytable==0.8.23a0 because these package versions have conflicting dependencies.\n",
    "\n",
    "The conflict is caused by:\n",
    "    pytable 0.8.23a0 depends on basicproperty>=0.6.9a\n",
    "    pytable 0.8.22a0 depends on basicproperty>=0.6.9a\n",
    "    pytable 0.8.21a0 depends on basicproperty>=0.6.9a\n",
    "\n",
    "To fix this you could try to:\n",
    "1. loosen the range of package versions you've specified\n",
    "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 다음은 `DataFrame`을 HDF5 포맷으로 작성하는 예다.  \n",
    " 출력 결과를 보면 HDF5 스토어에는 df라는 이름의 루트 객체가 있고, 이는 데이터 프레임을 의미하며, 그 모양은 8개의 로우와 3개의 칼럼이라는 정보를 보여준다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for replication\n",
    "np.random.seed(123456)\n",
    "# create a DataFrame of dates and random numbers in three columns\n",
    "df = pd.DataFrame(np.random.randn(8, 3), \n",
    "                  index= pd.date_range('1/1/2000', periods= 8),\n",
    "                  columns= ['A', 'B', 'C'])\n",
    "\n",
    "# create HDF5 store\n",
    "store = pd.HDFStore('/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/store.h5')\n",
    "store['df'] = df # persisting happened here\n",
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 HDF5 스토어를 DataFrame에 로딩하는 예다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from HDF5\n",
    "store = pd.HDFStore(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/store.h5\")\n",
    "df = store['df']\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame`은 스토어 객체에 할당되는 순간 HDF5 파일로 작성된다.  \n",
    "따라서 그 이후의 `DataFrame`에 대한 변경 사항은 저장되지 않는다. `DataFrame`을 스토어에 다시 할당하기 전까지는 말이다!  \n",
    "\n",
    "다음은 데이터 스토어를 갱신하는 예를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   A         B         C\n",
       "2000-01-01  1.000000 -0.282863 -1.509059\n",
       "2000-01-02 -1.135632  1.212112 -0.173215\n",
       "2000-01-03  0.119209 -1.044236 -0.861849\n",
       "2000-01-04 -2.104569 -0.494929  1.071804\n",
       "2000-01-05  0.721555 -0.706771 -1.039575"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this changes the DataFrame, but did not persist\n",
    "df.iloc[0].A = 1 \n",
    "# to persist the change, assign the DataFrame to the \n",
    "# HDF5 store object\n",
    "store['df'] = df\n",
    "\n",
    "# it is now persisted\n",
    "# the following loads the store and \n",
    "# shows the first two rows, demonstrating\n",
    "# the the persisting was done\n",
    "pd.HDFStore(\"/Users/Angela/Desktop/Personal/Learning-Pandas-Second-Edition-master/data/store.h5\")['df'][:5] # it's now in there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 데이터\n",
    "---\n",
    "## Accessing data on the web and in the cloud\n",
    "웹을 통한 CSV 데이터 접근\n",
    "\n",
    "인터넷을 통해 웹으로 부터 데이터를 읽어 들이는 일은 매우 흔한 작업이다. pandas는 그 작업을 쉽게 할 수 있게 지원한다.  \n",
    "지금까지 봤던 모든 pandas 함수는 로컬 파일 경로 대신 HTTP URL, FTP 주소, 심지어 Amazon S3 주소까지 가능하다. 또한 모든 작업은 로컬 파일로 작업할 때와 동일하다.  \n",
    "\n",
    "예제에서는 기존 `pd.read_csv()` 함수를 사용하여 HTTP 요청을 만드는 일이 **얼마나 쉬운지** 보여준다. 여기서는 HTTP 쿼리 문자열(query string) 방식으로 Google Finance 웹 서비스로부터 2017년 4월의 마이크로소프트 주식 데이터를 직접 가져오려고 했었다.  \n",
    "하지만 원본 코드(정확히는 적혀있는 주소..?)문제로 수정하였다. 야후 파이낸스에서 마이크로소프트 말고 인텔의 2022-11 한달 주식을 가져오려고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 원본 코드\n",
    "\n",
    "# # read csv directly from Yahoo! Finance from a URL\n",
    "# msft_hist = pd.read_csv(\n",
    "#     \"http://www.google.com/finance/historical?\" +\n",
    "#     \"q=NASDAQ:MSFT&startdate=Apr+01%2C+2017&\" +\n",
    "#     \"enddate=Apr+30%2C+2017&output=csv\")\n",
    "# msft_hist[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 주소와 토큰 코드\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# read csv directly from Yahoo! Finance from a URL\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m msft_hist \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://finance.yahoo.com/quote/INTC/history?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mq=INTC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     \"enddate=Apr+30%2C+2017&output=csv\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m msft_hist[:\u001b[38;5;241m5\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/pandas/io/common.py:670\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    667\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 670\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    679\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/pandas/io/common.py:339\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    338\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 339\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    340\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/pandas/io/common.py:239\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/urllib/request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    530\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 531\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/urllib/request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 640\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/urllib/request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    568\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/urllib/request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# 주소와 토큰 코드\n",
    "# read csv directly from Yahoo! Finance from a URL\n",
    "msft_hist = pd.read_csv(\n",
    "    \"https://finance.yahoo.com/quote/INTC/history?\" +\n",
    "    \"q=INTC\")\n",
    "#     \"enddate=Apr+30%2C+2017&output=csv\")\n",
    "msft_hist[:5]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
