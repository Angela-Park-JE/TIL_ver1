{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee0e18b",
   "metadata": {},
   "source": [
    "# 머신러닝 with Python \n",
    "## 09 앙상블 학습 보팅 실습\n",
    "\n",
    "##### 보팅 알고리즘을 활용해 꽃 데이터를 분류하는 모형을 생성해본다.\n",
    "\n",
    "책 p.259~\n",
    "\n",
    "---\n",
    "\n",
    "하단부에서는 모델별 voting weight 값을 변경해보면서 실험해 보았다.\n",
    "\n",
    "> 배운점1: voting 방식을 soft로 하려면 결과가 예측값이 아니라 예측 확률이 나와야 한다. (분류기에서 안됨)\n",
    "\n",
    ">  배운점2: 같은 분류기들로 이루어진 앙상블이라도 weight 값을 변경하면 정확도에 영향을 미칠 수 있다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022679e",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e3ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf788f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_iris = datasets.load_iris() # 꽃 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4be6bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 살펴보기\n",
    "\n",
    "raw_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0dae9",
   "metadata": {},
   "source": [
    "### 피처, 타깃 데이터 지정\n",
    "\n",
    "보통 train data는 행렬이므로 대문자 X, 예측되어야 할 값은 스칼라로 소문자 y가 된다는 것을 들은게 생각이 나부렀다.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76b530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_iris.data\n",
    "y = raw_iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ae465",
   "metadata": {},
   "source": [
    "### 트레이닝/테스트 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3acca5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split              # 분할을 위해 필요한 함수\n",
    "X_tn, X_te, y_tn, y_te = train_test_split(X, y, random_state = 0) # 분리. randomstate는 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc70ec",
   "metadata": {},
   "source": [
    "### 데이터 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a771eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # 데이터 표준화를 위한 함수\n",
    "std_scale = StandardScaler()                     # 표준화 스케일러 지정\n",
    "std_scale.fit(X_tn)                              # 트레이닝 피처를 기준으로 표준화를 적합시키기\n",
    "\n",
    "\n",
    "X_tn_std = std_scale.transform(X_tn) \n",
    "X_te_std = std_scale.transform(X_te)             # 트레인, 테스트 데이터 각각 적합시킨 표준화에 맞게 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226767f4",
   "metadata": {},
   "source": [
    "### 데이터 학습 - 보팅 학습\n",
    "\n",
    "이번 실습에서는 로지스틱 회귀분석, 가우시안 나이브 베이즈, 서포트 벡터 머신을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d447a0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(multi_class='multinomial',\n",
       "                                                 random_state=1)),\n",
       "                             ('svm', SVC(kernel='linear', random_state=1)),\n",
       "                             ('gnb', GaussianNB())],\n",
       "                 weights=[1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # 로지트틱 회귀분석\n",
    "from sklearn import svm                             # support vector machine\n",
    "from sklearn.naive_bayes import GaussianNB          # Gussian Naive Bayes\n",
    "from sklearn.ensemble import VotingClassifier       # 분류 문제이기 때문이 VotingClassifier를 사용. 회귀문제라면 VotingRegressor를 사용한다.\n",
    "\n",
    "clf1 = LogisticRegression(multi_class = 'multinomial', \n",
    "                          random_state = 1)         # 첫 번째 모형을 로지스틱 회귀분석이라고 정함\n",
    "clf2 = svm.SVC(kernel = 'linear', \n",
    "              random_state = 1)                     # 두 번째 모형을 서포트 벡터 머신 모형이라고 정함\n",
    "clf3 = GaussianNB()                                 # 세 번째 모형은 가우시안 나이브 베이즈\n",
    "\n",
    "clf_voting = VotingClassifier(estimators = [        # estimators : 미리 만든 세 가지 모형을 의미\n",
    "                                    ('lr', clf1),\n",
    "                                    ('svm', clf2),\n",
    "                                    ('gnb', clf3),\n",
    "                                                ],\n",
    "                             voting = 'hard',       # voting : hard(default)=과반수가 넘는 라벨, soft=확률이 가장 높은 라벨\n",
    "                             weights = [1, 1, 1])   # weights : 각 모형의 비율. 중요도 가중치를 어떻게 줄지인 것으로 예상됨. \n",
    "\n",
    "clf_voting.fit(X_tn_std, y_tn)                      # 만들어진 모형에 표준화된 트레이닝 피처 데이터와 트레이닝 타깃 데이터를 넣고 적합시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd9156",
   "metadata": {},
   "source": [
    "### 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3205bf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "pred_voting = clf_voting.predict(X_te_std)          # std된 테스트 피처 데이터를 넣고 실행\n",
    "print(pred_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5639cdb8",
   "metadata": {},
   "source": [
    "### 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901af3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_voting)        # 실제값과 예측값을 넣음\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5ea91",
   "metadata": {},
   "source": [
    "### confusion matrix 확인\n",
    "\n",
    "각 클래스를 각 클래스로 예측한 개수. 대부분 데이터가 잘 분리된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a91b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_te, pred_voting)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa8933",
   "metadata": {},
   "source": [
    "### 분류 리포트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "996e29e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_te, pred_voting)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe06f1",
   "metadata": {},
   "source": [
    "## 실험1- voting 방식 변경해보기 (soft로) -실패"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c26e9",
   "metadata": {},
   "source": [
    "### 데이터 학습 - 보팅 학습\n",
    "\n",
    "로지스틱 회귀분석, 가우시안 나이브 베이즈, 서포트 벡터 머신을 그대로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb45825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(multi_class='multinomial',\n",
       "                                                 random_state=1)),\n",
       "                             ('svm', SVC(kernel='linear', random_state=1)),\n",
       "                             ('gnb', GaussianNB())],\n",
       "                 voting='soft', weights=[1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # 로지트틱 회귀분석\n",
    "from sklearn import svm                             # support vector machine\n",
    "from sklearn.naive_bayes import GaussianNB          # Gussian Naive Bayes\n",
    "from sklearn.ensemble import VotingClassifier       # 분류 문제이기 때문이 VotingClassifier를 사용. 회귀문제라면 VotingRegressor를 사용한다.\n",
    "\n",
    "clf1 = LogisticRegression(multi_class = 'multinomial', \n",
    "                          random_state = 1)         # 첫 번째 모형을 로지스틱 회귀분석이라고 정함\n",
    "clf2 = svm.SVC(kernel = 'linear', \n",
    "              random_state = 1)                     # 두 번째 모형을 서포트 벡터 머신 모형이라고 정함\n",
    "clf3 = GaussianNB()                                 # 세 번째 모형은 가우시안 나이브 베이즈\n",
    "\n",
    "clf_voting = VotingClassifier(estimators = [        # estimators : 미리 만든 세 가지 모형을 의미\n",
    "                                    ('lr', clf1),\n",
    "                                    ('svm', clf2),\n",
    "                                    ('gnb', clf3),\n",
    "                                                ],\n",
    "                             voting = 'soft',       # voting : hard(default)=과반수가 넘는 라벨, soft=확률이 가장 높은 라벨\n",
    "                             weights = [1, 1, 1])   # weights : 각 모형의 비율. 중요도 가중치를 어떻게 줄지인 것으로 예상됨. \n",
    "\n",
    "clf_voting.fit(X_tn_std, y_tn)                      # 만들어진 모형에 표준화된 트레이닝 피처 데이터와 트레이닝 타깃 데이터를 넣고 적합시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752b517",
   "metadata": {},
   "source": [
    "### 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e6c38e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred_voting \u001b[38;5;241m=\u001b[39m \u001b[43mclf_voting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_te_std\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# std된 테스트 피처 데이터를 넣고 실행\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred_voting)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/sklearn/ensemble/_voting.py:341\u001b[0m, in \u001b[0;36mVotingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    339\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoting \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 341\u001b[0m     maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# 'hard' voting\u001b[39;00m\n\u001b[1;32m    344\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/sklearn/utils/metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/sklearn/ensemble/_voting.py:382\u001b[0m, in \u001b[0;36mVotingClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    Weighted average probability for each class per sample.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    380\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    381\u001b[0m avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_probas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_not_none\n\u001b[1;32m    383\u001b[0m )\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/sklearn/ensemble/_voting.py:357\u001b[0m, in \u001b[0;36mVotingClassifier._collect_probas\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([clf\u001b[38;5;241m.\u001b[39mpredict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/sklearn/ensemble/_voting.py:357\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/sklearn/utils/metaestimators.py:109\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m    103\u001b[0m attr_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(owner\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py3_8_5/lib/python3.8/site-packages/sklearn/svm/_base.py:800\u001b[0m, in \u001b[0;36mBaseSVC._check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability:\n\u001b[0;32m--> 800\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba is not available when  probability=False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m         )\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_svc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnu_svc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba only implemented for SVC and NuSVC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "pred_voting = clf_voting.predict(X_te_std)          # std된 테스트 피처 데이터를 넣고 실행\n",
    "print(pred_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4d737",
   "metadata": {},
   "source": [
    "아예 돌아가지 않는다.\n",
    "\n",
    "$ AttributeError: predict_proba is not available when  probability=False $\n",
    "\n",
    "아마도 분류가 된 모형에서 확률을 찾으려니까 안되는 것인가? 추측해본다.\n",
    "\n",
    "왜그럴까?\n",
    "\n",
    "---\n",
    "\n",
    "> *그런데 제가 제대로 알고 있는 유일한 모델인 Decision Tree는 답을 확률로 알려주는 것이 아니니 Soft Voting이 불가능할까요? 놀랍게도 scikit learn에서 제공하는 DecisionTreeClassifier에는 각 class의 확률을 계산해주는  predict_prob 함수가 있습니다. 확률을 계산하는 방법은 단순하게 train dataset(의 일부) 를 Decision Tree에 넣어보고 각 leaf로 나오는 값들 중 정확한 값인 비율을 계산합니다* *참조: https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting*\n",
    "\n",
    "라는 말이 있다. 이런 부분을 미루어 보았을 떄 위에서 사용한 모델들에서의 결과는 \n",
    "\n",
    "```\n",
    "pred_voting = clf_voting.predict(X_te_std)          # std된 테스트 피처 데이터를 넣고 실행\n",
    "print(pred_voting)\n",
    "```\n",
    "라고 했을 때의 결과처럼 아예 어떤 클래스에 속하는지 라벨을 뱉는다. 아마 그 부분의 문제인 것 같다.\n",
    "\n",
    "실제로 위 글에서도 'Decision Tree Classifier의 Probability 계산'이라는 제목의 문단에 코드로 이렇게 적혀있다\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iowa_model2 = DecisionTreeClassifier()\n",
    "iowa_model2.fit(X, y)\n",
    "predictions = iowa_model2.predict_proba(X)\n",
    "print(predictions.tolist())\n",
    "```\n",
    "\n",
    "predict_prob 함수를 사용해서 확률을 계산한다고 했는데... 생각한 이유가 얼추 맞는 것 같다는 느낌이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8c417",
   "metadata": {},
   "source": [
    "## 실험2- weight 값 변경하기\n",
    "(lr = 2, svm = 1, gnb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567e956",
   "metadata": {},
   "source": [
    "### 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54adf90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(multi_class='multinomial',\n",
       "                                                 random_state=1)),\n",
       "                             ('svm', SVC(kernel='linear', random_state=1)),\n",
       "                             ('gnb', GaussianNB())],\n",
       "                 weights=[2, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # 로지트틱 회귀분석\n",
    "from sklearn import svm                             # support vector machine\n",
    "from sklearn.naive_bayes import GaussianNB          # Gussian Naive Bayes\n",
    "from sklearn.ensemble import VotingClassifier       # 분류 문제이기 때문이 VotingClassifier를 사용. 회귀문제라면 VotingRegressor를 사용한다.\n",
    "\n",
    "clf1 = LogisticRegression(multi_class = 'multinomial', \n",
    "                          random_state = 1)         # 첫 번째 모형을 로지스틱 회귀분석이라고 정함\n",
    "clf2 = svm.SVC(kernel = 'linear', \n",
    "              random_state = 1)                     # 두 번째 모형을 서포트 벡터 머신 모형이라고 정함\n",
    "clf3 = GaussianNB()                                 # 세 번째 모형은 가우시안 나이브 베이즈\n",
    "\n",
    "clf_voting = VotingClassifier(estimators = [        # estimators : 미리 만든 세 가지 모형을 의미\n",
    "                                    ('lr', clf1),\n",
    "                                    ('svm', clf2),\n",
    "                                    ('gnb', clf3),\n",
    "                                                ],\n",
    "                             voting = 'hard',       # voting : hard(default)=과반수가 넘는 라벨, soft=확률이 가장 높은 라벨\n",
    "                             weights = [2, 1, 1])   # weights : 각 모형의 비율. 중요도 가중치를 어떻게 줄지인 것으로 예상됨. \n",
    "\n",
    "clf_voting.fit(X_tn_std, y_tn)                      # 만들어진 모형에 표준화된 트레이닝 피처 데이터와 트레이닝 타깃 데이터를 넣고 적합시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25579740",
   "metadata": {},
   "source": [
    "### 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2adaa606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "pred_voting = clf_voting.predict(X_te_std)          # std된 테스트 피처 데이터를 넣고 실행\n",
    "print(pred_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb2f28",
   "metadata": {},
   "source": [
    "### 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f444b616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_voting)        # 실제값과 예측값을 넣음\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89523e9",
   "metadata": {},
   "source": [
    "### confusion matrix 확인\n",
    "\n",
    "각 클래스를 각 클래스로 예측한 개수. 대부분 데이터가 잘 분리된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b42abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_te, pred_voting)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92e3b0",
   "metadata": {},
   "source": [
    "### 분류 리포트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0cf5f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_te, pred_voting)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67c1d8",
   "metadata": {},
   "source": [
    "## 실험3- weight 값 변경하기\n",
    "(lr = 1, svm = 3, gnb = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b2cc9",
   "metadata": {},
   "source": [
    "### 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1390ebad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(multi_class='multinomial',\n",
       "                                                 random_state=1)),\n",
       "                             ('svm', SVC(kernel='linear', random_state=1)),\n",
       "                             ('gnb', GaussianNB())],\n",
       "                 weights=[1, 3, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # 로지트틱 회귀분석\n",
    "from sklearn import svm                             # support vector machine\n",
    "from sklearn.naive_bayes import GaussianNB          # Gussian Naive Bayes\n",
    "from sklearn.ensemble import VotingClassifier       # 분류 문제이기 때문이 VotingClassifier를 사용. 회귀문제라면 VotingRegressor를 사용한다.\n",
    "\n",
    "clf1 = LogisticRegression(multi_class = 'multinomial', \n",
    "                          random_state = 1)         # 첫 번째 모형을 로지스틱 회귀분석이라고 정함\n",
    "clf2 = svm.SVC(kernel = 'linear', \n",
    "              random_state = 1)                     # 두 번째 모형을 서포트 벡터 머신 모형이라고 정함\n",
    "clf3 = GaussianNB()                                 # 세 번째 모형은 가우시안 나이브 베이즈\n",
    "\n",
    "clf_voting = VotingClassifier(estimators = [        # estimators : 미리 만든 세 가지 모형을 의미\n",
    "                                    ('lr', clf1),\n",
    "                                    ('svm', clf2),\n",
    "                                    ('gnb', clf3),\n",
    "                                                ],\n",
    "                             voting = 'hard',       # voting : hard(default)=과반수가 넘는 라벨, soft=확률이 가장 높은 라벨\n",
    "                             weights = [1, 3, 1])   # weights : 각 모형의 비율. 중요도 가중치를 어떻게 줄지인 것으로 예상됨. \n",
    "\n",
    "clf_voting.fit(X_tn_std, y_tn)                      # 만들어진 모형에 표준화된 트레이닝 피처 데이터와 트레이닝 타깃 데이터를 넣고 적합시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4763e8",
   "metadata": {},
   "source": [
    "### 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4dfd97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "pred_voting = clf_voting.predict(X_te_std)          # std된 테스트 피처 데이터를 넣고 실행\n",
    "print(pred_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafcacaf",
   "metadata": {},
   "source": [
    "### 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2da1aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_voting)        # 실제값과 예측값을 넣음\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a4d9a",
   "metadata": {},
   "source": [
    "### confusion matrix 확인\n",
    "\n",
    "각 클래스를 각 클래스로 예측한 개수. 대부분 데이터가 잘 분리된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a80b197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_te, pred_voting)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c440cf",
   "metadata": {},
   "source": [
    "### 분류 리포트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1adaead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_te, pred_voting)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3c73a",
   "metadata": {},
   "source": [
    "너무 단순한 데이터로 한게 문제인 걸까... 다른게 없네."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1711c0a",
   "metadata": {},
   "source": [
    "## 실험4- weight 값 변경하기(정확도 100 나온 사례)\n",
    "(lr = 1, svm = 1, gnb = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb3639",
   "metadata": {},
   "source": [
    "### 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62ca0d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(multi_class='multinomial',\n",
       "                                                 random_state=1)),\n",
       "                             ('svm', SVC(kernel='linear', random_state=1)),\n",
       "                             ('gnb', GaussianNB())],\n",
       "                 weights=[1, 1, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # 로지트틱 회귀분석\n",
    "from sklearn import svm                             # support vector machine\n",
    "from sklearn.naive_bayes import GaussianNB          # Gussian Naive Bayes\n",
    "from sklearn.ensemble import VotingClassifier       # 분류 문제이기 때문이 VotingClassifier를 사용. 회귀문제라면 VotingRegressor를 사용한다.\n",
    "\n",
    "clf1 = LogisticRegression(multi_class = 'multinomial', \n",
    "                          random_state = 1)         # 첫 번째 모형을 로지스틱 회귀분석이라고 정함\n",
    "clf2 = svm.SVC(kernel = 'linear', \n",
    "              random_state = 1)                     # 두 번째 모형을 서포트 벡터 머신 모형이라고 정함\n",
    "clf3 = GaussianNB()                                 # 세 번째 모형은 가우시안 나이브 베이즈\n",
    "\n",
    "clf_voting = VotingClassifier(estimators = [        # estimators : 미리 만든 세 가지 모형을 의미\n",
    "                                    ('lr', clf1),\n",
    "                                    ('svm', clf2),\n",
    "                                    ('gnb', clf3),\n",
    "                                                ],\n",
    "                             voting = 'hard',       # voting : hard(default)=과반수가 넘는 라벨, soft=확률이 가장 높은 라벨\n",
    "                             weights = [1, 1, 8])   # weights : 각 모형의 비율. 중요도 가중치를 어떻게 줄지인 것으로 예상됨. \n",
    "\n",
    "clf_voting.fit(X_tn_std, y_tn)                      # 만들어진 모형에 표준화된 트레이닝 피처 데이터와 트레이닝 타깃 데이터를 넣고 적합시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8195fb1",
   "metadata": {},
   "source": [
    "### 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7764a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "pred_voting = clf_voting.predict(X_te_std)          # std된 테스트 피처 데이터를 넣고 실행\n",
    "print(pred_voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0e13c",
   "metadata": {},
   "source": [
    "### 정확도 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f643545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_te, pred_voting)        # 실제값과 예측값을 넣음\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f7d15",
   "metadata": {},
   "source": [
    "### confusion matrix 확인\n",
    "\n",
    "각 클래스를 각 클래스로 예측한 개수. 대부분 데이터가 잘 분리된 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7bba6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_te, pred_voting)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c871b",
   "metadata": {},
   "source": [
    "### 분류 리포트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e74dac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_te, pred_voting)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec62873",
   "metadata": {},
   "source": [
    "정확도가 100이 나오셨어요....?.....??????????\n",
    "\n",
    "직감적으로 100%라는 것은 굉장히 안좋은 숫자라고 느껴진다. 다른 데이터가 들어오면 정확도가 순식간에 떨어질 것이 불보듯 뻔하다고 해야하나. 재미있는 실험이었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
